{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract gaze features in VidHOI dataset\n",
    "Use head detection and gaze following methods to extract feature maps for all key frames in VidHOI dataset. Check face is inside a person bbox. Store them in a separate buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../modules/object_tracking/yolov5\")\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import shelve\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modules.object_tracking import HeadDetection\n",
    "from modules.gaze_following import GazeFollowing\n",
    "from modules.gaze_following.head_association import assign_human_head_video\n",
    "from common.action_genome_dataset import AGDataset\n",
    "from common.transforms import YOLOv5Transform\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Head Tracking and Gaze Following modules\n",
    "head_detection_module = HeadDetection(\n",
    "    crowd_human_weight_path=\"../weights/yolov5/crowdhuman_yolov5m.pt\",\n",
    "    config_path=\"../configs/object_tracking.yaml\",\n",
    "    device=device,\n",
    ")\n",
    "gaze_following_module = GazeFollowing(\n",
    "    weight_path=\"../weights/detecting_attended/model_videoatttarget.pt\",\n",
    "    config_path=\"../configs/gaze_following.yaml\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "img_size = 640\n",
    "yolov5_stride = head_detection_module.yolov5_stride\n",
    "# NOTE adjust this tolerance and method\n",
    "head_matching_iou_thres = 0.7\n",
    "head_matching_method = \"hungarian\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_val_dataset = AGDataset(\n",
    "    dataset_dir=\"/mnt/DATA/datasets/action_genome\",\n",
    "    mode=\"test\",\n",
    "    transform=YOLOv5Transform(img_size, yolov5_stride),\n",
    "    min_length=1,\n",
    "    max_length=999999,\n",
    "    annotation_mode=\"clip\",\n",
    ")\n",
    "ag_val_dataloader = DataLoader(ag_val_dataset, batch_size=None, shuffle=False)\n",
    "output_val_head_filename = \"/mnt/DATA/datasets/action_genome/action_genome_gaze/val_frame_heads_gt_bbox\"\n",
    "output_val_gaze_filename = \"/mnt/DATA/datasets/action_genome/action_genome_gaze/val_frame_gazes_gt_bbox\"\n",
    "output_val_inout_filename = \"/mnt/DATA/datasets/action_genome/action_genome_gaze/val_frame_inout_gt_bbox\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_val_head_dict = shelve.open(output_val_head_filename)\n",
    "output_val_gaze_dict = shelve.open(output_val_gaze_filename)\n",
    "output_val_inout_dict = shelve.open(output_val_inout_filename)\n",
    "# For each video, first detect heads\n",
    "t = tqdm(ag_val_dataloader)\n",
    "for frames, annotations, meta_info in t:\n",
    "    video_name = meta_info['video_name']\n",
    "    t.set_description(f\"{video_name}\")\n",
    "    t.refresh()\n",
    "    original_frames = meta_info[\"original_frames\"]\n",
    "    bboxes = annotations[\"bboxes\"]\n",
    "    ids = annotations[\"ids\"]\n",
    "    labels = annotations[\"labels\"]\n",
    "    bboxes = np.array(bboxes)\n",
    "    ids = np.array(ids)\n",
    "    labels = np.array(labels)\n",
    "    with torch.no_grad():\n",
    "        video_head_bbox_list = assign_human_head_video(\n",
    "            frames,\n",
    "            original_frames,\n",
    "            bboxes,\n",
    "            ids,\n",
    "            labels,\n",
    "            head_detection_module,\n",
    "            head_matching_iou_thres,\n",
    "            device,\n",
    "            method=head_matching_method,\n",
    "            human_label=1,\n",
    "        )\n",
    "    # assign video head bbox list to its name\n",
    "    output_val_head_dict[video_name] = video_head_bbox_list\n",
    "    output_val_head_dict.sync()\n",
    "\n",
    "    # for each head bbox, detect gaze\n",
    "    video_gaze_list = []\n",
    "    video_inout_list = []\n",
    "    hx_memory = {}\n",
    "    for i, (head_bboxes, frame0) in enumerate(\n",
    "        zip(video_head_bbox_list, original_frames)\n",
    "    ):\n",
    "        t.set_description(f\"{video_name}/{meta_info['frame_ids'][i]}, {i}/{len(video_head_bbox_list) - 1}: \")\n",
    "        t.refresh()\n",
    "        frame_gaze_dict = {}\n",
    "        frame_inout_dict = {}\n",
    "        for human_id, head_bbox in head_bboxes.items():\n",
    "            t.set_postfix_str(f\"{head_bbox}\")\n",
    "            # no head found for this human_id\n",
    "            if len(head_bbox) == 0:\n",
    "                frame_gaze_dict[human_id] = []\n",
    "                frame_inout_dict[human_id] = []\n",
    "                continue\n",
    "            # check hidden state memory\n",
    "            if human_id in hx_memory:\n",
    "                hidden_state = hx_memory[human_id]\n",
    "            else:\n",
    "                hidden_state = None\n",
    "            with torch.no_grad():\n",
    "                (heatmap, inout, hx, _, _, _,) = gaze_following_module.detect_one(\n",
    "                    frame0.numpy(),\n",
    "                    head_bbox,\n",
    "                    hidden_state,\n",
    "                    draw=False,\n",
    "                )\n",
    "            hx_memory[human_id] = (hx[0].detach(), hx[1].detach())\n",
    "            # process heatmap 64x64 (not include inout), store inout info separately\n",
    "            # softmax inout, value = probability of gaze inside the scene\n",
    "            inout_modulated = 1 / (1 + np.exp(-inout))\n",
    "            # inout_modulated = 1 - inout_modulated\n",
    "            # heatmap_modulated = heatmap - inout_modulated\n",
    "            # assign heatmap and in_out to human_id\n",
    "            frame_gaze_dict[human_id] = heatmap\n",
    "            frame_inout_dict[human_id] = inout_modulated\n",
    "        # append frame heatmap and inout dict to video heatmap list\n",
    "        video_gaze_list.append(frame_gaze_dict)\n",
    "        video_inout_list.append(frame_inout_dict)\n",
    "    # assign video heatmap list to its name\n",
    "    output_val_gaze_dict[video_name] = video_gaze_list\n",
    "    output_val_gaze_dict.sync()\n",
    "    output_val_inout_dict[video_name] = video_inout_list\n",
    "    output_val_inout_dict.sync()\n",
    "\n",
    "output_val_head_dict.close()\n",
    "output_val_gaze_dict.close()\n",
    "output_val_inout_dict.close()\n",
    "print(f\"Head bboxes dumped to {output_val_head_filename}\")\n",
    "print(f\"Gaze heatmaps dumped to {output_val_gaze_filename}\")\n",
    "print(f\"Gaze inout dumped to {output_val_inout_filename}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "ag_train_dataset = AGDataset(\n",
    "    dataset_dir=\"/mnt/DATA/datasets/action_genome\",\n",
    "    mode=\"train\",\n",
    "    transform=YOLOv5Transform(img_size, yolov5_stride),\n",
    "    min_length=1,\n",
    "    max_length=999999,\n",
    "    annotation_mode=\"clip\",\n",
    ")\n",
    "ag_train_dataloader = DataLoader(\n",
    "    ag_train_dataset, batch_size=None, shuffle=False\n",
    ")\n",
    "output_train_head_filename = \"/mnt/DATA/datasets/action_genome/action_genome_gaze/train_frame_heads_gt_bbox\"\n",
    "output_train_gaze_filename = \"/mnt/DATA/datasets/action_genome/action_genome_gaze/train_frame_gazes_gt_bbox\"\n",
    "output_train_inout_filename = \"/mnt/DATA/datasets/action_genome/action_genome_gaze/train_frame_inout_gt_bbox\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_head_dict = shelve.open(output_train_head_filename)\n",
    "output_train_gaze_dict = shelve.open(output_train_gaze_filename)\n",
    "output_train_inout_dict = shelve.open(output_train_inout_filename)\n",
    "# For each video, first detect heads\n",
    "t = tqdm(ag_train_dataloader)\n",
    "for frames, annotations, meta_info in t:\n",
    "    video_name = meta_info['video_name']\n",
    "    t.set_description(f\"{video_name}\")\n",
    "    t.refresh()\n",
    "    original_frames = meta_info[\"original_frames\"]\n",
    "    bboxes = annotations[\"bboxes\"]\n",
    "    ids = annotations[\"ids\"]\n",
    "    labels = annotations[\"labels\"]\n",
    "    bboxes = np.array(bboxes)\n",
    "    ids = np.array(ids)\n",
    "    labels = np.array(labels)\n",
    "    with torch.no_grad():\n",
    "        video_head_bbox_list = assign_human_head_video(\n",
    "            frames,\n",
    "            original_frames,\n",
    "            bboxes,\n",
    "            ids,\n",
    "            labels,\n",
    "            head_detection_module,\n",
    "            head_matching_iou_thres,\n",
    "            device,\n",
    "            method=head_matching_method,\n",
    "            human_label=1,\n",
    "        )\n",
    "    # assign video head bbox list to its name\n",
    "    output_train_head_dict[video_name] = video_head_bbox_list\n",
    "    output_train_head_dict.sync()\n",
    "\n",
    "    # for each head bbox, detect gaze\n",
    "    video_gaze_list = []\n",
    "    video_inout_list = []\n",
    "    hx_memory = {}\n",
    "    for i, (head_bboxes, frame0) in enumerate(\n",
    "        zip(video_head_bbox_list, original_frames)\n",
    "    ):\n",
    "        t.set_description(f\"{video_name}/{meta_info['frame_ids'][i]}, {i}/{len(video_head_bbox_list) - 1}: \")\n",
    "        t.refresh()\n",
    "        frame_gaze_dict = {}\n",
    "        frame_inout_dict = {}\n",
    "        for human_id, head_bbox in head_bboxes.items():\n",
    "            t.set_postfix_str(f\"{head_bbox}\")\n",
    "            # no head found for this human_id\n",
    "            if len(head_bbox) == 0:\n",
    "                frame_gaze_dict[human_id] = []\n",
    "                frame_inout_dict[human_id] = []\n",
    "                continue\n",
    "            # check hidden state memory\n",
    "            if human_id in hx_memory:\n",
    "                hidden_state = hx_memory[human_id]\n",
    "            else:\n",
    "                hidden_state = None\n",
    "            with torch.no_grad():\n",
    "                (heatmap, inout, hx, _, _, _,) = gaze_following_module.detect_one(\n",
    "                    frame0.numpy(),\n",
    "                    head_bbox,\n",
    "                    hidden_state,\n",
    "                    draw=False,\n",
    "                )\n",
    "            hx_memory[human_id] = (hx[0].detach(), hx[1].detach())\n",
    "            # process heatmap 64x64 (not include inout), store inout info separately\n",
    "            # softmax inout, value = probability of gaze inside the scene\n",
    "            inout_modulated = 1 / (1 + np.exp(-inout))\n",
    "            # inout_modulated = 1 - inout_modulated\n",
    "            # heatmap_modulated = heatmap - inout_modulated\n",
    "            # assign heatmap and in_out to human_id\n",
    "            frame_gaze_dict[human_id] = heatmap\n",
    "            frame_inout_dict[human_id] = inout_modulated\n",
    "        # append frame heatmap and inout dict to video heatmap list\n",
    "        video_gaze_list.append(frame_gaze_dict)\n",
    "        video_inout_list.append(frame_inout_dict)\n",
    "    # assign video heatmap list to its name\n",
    "    output_train_gaze_dict[video_name] = video_gaze_list\n",
    "    output_train_gaze_dict.sync()\n",
    "    output_train_inout_dict[video_name] = video_inout_list\n",
    "    output_train_inout_dict.sync()\n",
    "\n",
    "output_train_head_dict.close()\n",
    "output_train_gaze_dict.close()\n",
    "output_train_inout_dict.close()\n",
    "print(f\"Head bboxes dumped to {output_train_head_filename}\")\n",
    "print(f\"Gaze heatmaps dumped to {output_train_gaze_filename}\")\n",
    "print(f\"Gaze inout dumped to {output_train_inout_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hoi_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10be537dd544c2db9f613ac2c2c3348a8740d82fea605a7c3e8129db29e3148b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
