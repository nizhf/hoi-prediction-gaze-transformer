{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection in VidHOI Validation Set\n",
    "Detect and track objects in VidHOI validation dataset, store them in a buffer. Then apply gaze following method to extract gaze heatmaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../modules/object_tracking/yolov5\")\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import shelve\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modules.object_tracking import HeadDetection, ObjectTracking\n",
    "from modules.gaze_following import GazeFollowing\n",
    "from modules.gaze_following.head_association import assign_human_head_video\n",
    "from common.vidhoi_dataset import VidHOIDataset\n",
    "from common.data_io import FrameDatasetLoader\n",
    "from common.transforms import YOLOv5Transform\n",
    "from common.image_processing import convert_annotation_frame_to_video\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# dataset_folder = Path(\"G:/datasets/VidOR\")\n",
    "dataset_folder = Path(\"/mnt/DATA/datasets/VidOR\")\n",
    "\n",
    "output_folder = dataset_folder / \"VidHOI_detection\"\n",
    "\n",
    "yolov5_model_size = \"yolov5l\"\n",
    "\n",
    "# tracking_mode = \"key\"  # only track objects in key frames\n",
    "tracking_mode = \"all\"  # track objects in all frames, then only keep the key frames\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Tracking Module init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_tracking_module = ObjectTracking(\n",
    "    yolo_weights_path=\"../weights/yolov5/vidor_\" + yolov5_model_size + \".pt\",\n",
    "    deep_sort_model_dir=\"../weights/deep_sort/\",\n",
    "    config_path=\"../configs/object_tracking.yaml\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "img_size = 640\n",
    "yolov5_stride = object_tracking_module.yolov5_stride\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset init, only validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidhoi_val_dataset = VidHOIDataset(\n",
    "    annotations_file=dataset_folder / \"VidHOI_annotation\" / \"val_frame_annots.json\",\n",
    "    frames_dir=dataset_folder / \"images\",\n",
    "    transform=YOLOv5Transform(img_size, yolov5_stride),\n",
    "    min_length=1,\n",
    "    max_length=999999,\n",
    "    max_human_num=999999,\n",
    "    annotation_mode=\"clip\",\n",
    "    train_ratio=0,\n",
    ")\n",
    "vidhoi_val_dataset.eval()\n",
    "vidhoi_val_dataloader = DataLoader(vidhoi_val_dataset, batch_size=None, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Tracking\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only key frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tracking_mode == \"key\":\n",
    "    # dict for all videos\n",
    "    all_detections = {}\n",
    "    t = tqdm(vidhoi_val_dataloader)\n",
    "    # for each video\n",
    "    # don't need annotation here\n",
    "    for frames, _, meta_info in t:\n",
    "        video_name = meta_info['video_name']\n",
    "        t.set_description(f\"{video_name}\")\n",
    "        t.refresh()\n",
    "        original_frames = meta_info[\"original_frames\"]\n",
    "        frame_ids = meta_info[\"frame_ids\"]\n",
    "        clip_len = len(frames) - 1\n",
    "        # object tracking init\n",
    "        object_tracking_module.clear()\n",
    "        object_tracking_module.warmup(frames[0].to(device), original_frames[0])\n",
    "        # entry for one video\n",
    "        clip_detections = {\n",
    "            \"bboxes\": [],\n",
    "            \"ids\": [],\n",
    "            \"labels\": [],\n",
    "            \"confidences\": [],\n",
    "            \"frame_ids\": [],\n",
    "        }\n",
    "        # for each frame, do detection and tracking\n",
    "        for im_idx, (frame, original_frame, frame_id) in enumerate(zip(frames, original_frames, frame_ids)):\n",
    "            t.set_postfix_str(f\"{im_idx}/{clip_len}: {frame_id}\")\n",
    "            t.refresh()\n",
    "            bboxes, ids, labels, _, confidences, _ = object_tracking_module.track_one(frame.to(device), original_frame, draw=False)\n",
    "            # frame-based format, NOTE need to convert to [im_idx, x1, y1, x2, y2] later\n",
    "            bboxes = [bbox.tolist() for bbox in bboxes]\n",
    "            clip_detections[\"bboxes\"].append(bboxes)\n",
    "            clip_detections[\"ids\"].append(ids)\n",
    "            clip_detections[\"labels\"].append(labels)\n",
    "            clip_detections[\"confidences\"].append(confidences)\n",
    "            clip_detections[\"frame_ids\"].append(frame_id)\n",
    "        all_detections[video_name] = clip_detections\n",
    "else:\n",
    "    print(\"Skip, not key frame mode\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All frames mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tracking_mode == \"all\":\n",
    "    total_frame_num = 0\n",
    "    # dict for all videos\n",
    "    all_detections = {}\n",
    "    # for each video, load all frames, only keep the detections in key frames\n",
    "    # don't need annotation here\n",
    "    t = tqdm(range(len(vidhoi_val_dataset)))\n",
    "    for video_idx in t:\n",
    "        video_name = vidhoi_val_dataset.video_name_list[video_idx]\n",
    "        frame_ids = vidhoi_val_dataset.frame_ids_list[video_idx]\n",
    "        t.set_description(f\"{video_name}\")\n",
    "        t.refresh()\n",
    "        # entry for one video\n",
    "        clip_detections = {\n",
    "            \"bboxes\": [],\n",
    "            \"ids\": [],\n",
    "            \"labels\": [],\n",
    "            \"confidences\": [],\n",
    "            \"frame_ids\": [],\n",
    "        }\n",
    "        # load all frames\n",
    "        video_frame_path = dataset_folder / \"images\" / video_name\n",
    "        video_loader = FrameDatasetLoader(video_frame_path, YOLOv5Transform(img_size, yolov5_stride))\n",
    "        for frame_idx, (frame, frame0, _, _, meta_info) in enumerate(video_loader):\n",
    "            total_frame_num += 1\n",
    "            if frame_idx == 0:\n",
    "                # object tracking init\n",
    "                object_tracking_module.clear()\n",
    "                object_tracking_module.warmup(frame.to(device), frame0)\n",
    "                \n",
    "            frame_id = str(meta_info[\"frame_path\"])[-10:-4]\n",
    "            clip_len = meta_info[\"frame_num\"] - 1\n",
    "            t.set_postfix_str(f\"{frame_idx}/{clip_len}: {frame_id}\")\n",
    "            t.refresh()\n",
    "            bboxes, ids, labels, _, confidences, _ = object_tracking_module.track_one(frame.to(device), frame0, draw=False)\n",
    "            # only store the detections in key frame set\n",
    "            if frame_id in frame_ids:\n",
    "                # frame-based format, NOTE need to convert to [im_idx, x1, y1, x2, y2] later\n",
    "                bboxes = [bbox.tolist() for bbox in bboxes]\n",
    "                clip_detections[\"bboxes\"].append(bboxes)\n",
    "                clip_detections[\"ids\"].append(ids)\n",
    "                clip_detections[\"labels\"].append(labels)\n",
    "                clip_detections[\"confidences\"].append(confidences)\n",
    "                clip_detections[\"frame_ids\"].append(frame_id)\n",
    "        all_detections[video_name] = clip_detections\n",
    "    print(f\"\\nTotally {total_frame_num} frames\")\n",
    "else:\n",
    "    print(\"Skip, not all frame mode\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "filename = output_folder / (\"val_trace_\" + yolov5_model_size + \"_deepsort.json\")\n",
    "out_str = json.dumps(all_detections)\n",
    "with filename.open(\"w\") as out_file:\n",
    "    out_file.write(out_str)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaze Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Head Tracking and Gaze Following modules\n",
    "head_detection_module = HeadDetection(\n",
    "    crowd_human_weight_path=\"../weights/yolov5/crowdhuman_yolov5m.pt\",\n",
    "    config_path=\"../configs/object_tracking.yaml\",\n",
    "    device=device,\n",
    ")\n",
    "gaze_following_module = GazeFollowing(\n",
    "    weight_path=\"../weights/detecting_attended/model_videoatttarget.pt\",\n",
    "    config_path=\"../configs/gaze_following.yaml\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "img_size = 640\n",
    "yolov5_stride = head_detection_module.yolov5_stride\n",
    "# NOTE adjust this tolerance and method\n",
    "head_matching_iou_thres = 0.7\n",
    "head_matching_method = \"hungarian\"\n",
    "# head_matching_method = \"greedy\"\n",
    "\n",
    "output_val_head_filename = str(output_folder / (\"val_frame_heads_\" + yolov5_model_size + \"_deepsort\"))\n",
    "output_val_gaze_filename = str(output_folder / (\"val_frame_gazes_\" + yolov5_model_size + \"_deepsort\"))\n",
    "output_val_inout_filename = str(output_folder / (\"val_frame_inout_\" + yolov5_model_size + \"_deepsort\"))\n",
    "\n",
    "filename = output_folder / (\"val_trace_\" + yolov5_model_size + \"_deepsort.json\")\n",
    "with filename.open() as detection_file:\n",
    "    all_detections = json.loads(detection_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_val_head_dict = shelve.open(output_val_head_filename)\n",
    "output_val_gaze_dict = shelve.open(output_val_gaze_filename)\n",
    "output_val_inout_dict = shelve.open(output_val_inout_filename)\n",
    "# For each video, first detect heads\n",
    "t = tqdm(vidhoi_val_dataloader)\n",
    "for frames, annotations, meta_info in t:\n",
    "    video_name = meta_info['video_name']\n",
    "    t.set_description(f\"{video_name}\")\n",
    "    t.refresh()\n",
    "    original_frames = meta_info[\"original_frames\"]\n",
    "    # bboxes from detection\n",
    "    clip_detections = all_detections[video_name]\n",
    "    # convert to [im_idx, x1, y1, x2, y2] format\n",
    "    bboxes, ids, labels, _ = convert_annotation_frame_to_video(clip_detections[\"bboxes\"], clip_detections[\"ids\"], clip_detections[\"labels\"], clip_detections[\"confidences\"])\n",
    "    # detect head and assign to human\n",
    "    with torch.no_grad():\n",
    "        video_head_bbox_list = assign_human_head_video(\n",
    "            frames,\n",
    "            original_frames,\n",
    "            bboxes,\n",
    "            ids,\n",
    "            labels,\n",
    "            head_detection_module,\n",
    "            head_matching_iou_thres,\n",
    "            device,\n",
    "            method=head_matching_method,\n",
    "        )\n",
    "    # assign video head bbox list to its name\n",
    "    output_val_head_dict[video_name] = video_head_bbox_list\n",
    "    output_val_head_dict.sync()\n",
    "\n",
    "    # for each head bbox, detect gaze\n",
    "    video_gaze_list = []\n",
    "    video_inout_list = []\n",
    "    hx_memory = {}\n",
    "    for i, (head_bboxes, frame0) in enumerate(\n",
    "        zip(video_head_bbox_list, original_frames)\n",
    "    ):\n",
    "        t.set_description(f\"{video_name}/{meta_info['frame_ids'][i]}, {i}/{len(video_head_bbox_list) - 1}: \")\n",
    "        t.refresh()\n",
    "        frame_gaze_dict = {}\n",
    "        frame_inout_dict = {}\n",
    "        for human_id, head_bbox in head_bboxes.items():\n",
    "            t.set_postfix_str(f\"{head_bbox}\")\n",
    "            t.refresh()\n",
    "            # no head found for this human_id\n",
    "            if len(head_bbox) == 0:\n",
    "                frame_gaze_dict[human_id] = []\n",
    "                frame_inout_dict[human_id] = []\n",
    "                continue\n",
    "            # check hidden state memory\n",
    "            if human_id in hx_memory:\n",
    "                hidden_state = hx_memory[human_id]\n",
    "            else:\n",
    "                hidden_state = None\n",
    "            with torch.no_grad():\n",
    "                (heatmap, inout, hx, _, _, _,) = gaze_following_module.detect_one(\n",
    "                    frame0.numpy(),\n",
    "                    head_bbox,\n",
    "                    hidden_state,\n",
    "                    draw=False,\n",
    "                )\n",
    "            hx_memory[human_id] = (hx[0].detach(), hx[1].detach())\n",
    "            # process heatmap 64x64 (not include inout), store inout info separately\n",
    "            # softmax inout, value = probability of gaze inside the scene\n",
    "            inout_modulated = 1 / (1 + np.exp(-inout))\n",
    "            # assign heatmap and in_out to human_id\n",
    "            frame_gaze_dict[human_id] = heatmap\n",
    "            frame_inout_dict[human_id] = inout_modulated\n",
    "        # append frame heatmap and inout dict to video heatmap list\n",
    "        video_gaze_list.append(frame_gaze_dict)\n",
    "        video_inout_list.append(frame_inout_dict)\n",
    "    # assign video heatmap list to its name\n",
    "    output_val_gaze_dict[video_name] = video_gaze_list\n",
    "    output_val_gaze_dict.sync()\n",
    "    output_val_inout_dict[video_name] = video_inout_list\n",
    "    output_val_inout_dict.sync()\n",
    "\n",
    "output_val_head_dict.close()\n",
    "output_val_gaze_dict.close()\n",
    "output_val_inout_dict.close()\n",
    "print(f\"Head bboxes dumped to {output_val_head_filename}\")\n",
    "print(f\"Gaze heatmaps dumped to {output_val_gaze_filename}\")\n",
    "print(f\"Gaze inout dumped to {output_val_inout_filename}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10be537dd544c2db9f613ac2c2c3348a8740d82fea605a7c3e8129db29e3148b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hoi_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
