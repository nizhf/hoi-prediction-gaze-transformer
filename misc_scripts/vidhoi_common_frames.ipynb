{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../modules/object_tracking/yolov5\")\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common.vidhoi_dataset import VidHOIDataset, dataset_collate_fn\n",
    "from common.transforms import YOLOv5Transform, STTranTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidhoi_dataset_val1 = VidHOIDataset(\n",
    "    annotations_file=\"G:/datasets/VidOR/VidHOI_annotation/val_frame_annots.json\",\n",
    "    frames_dir=\"G:/datasets/VidOR/images\",\n",
    "    min_length=1,\n",
    "    max_length=6,\n",
    "    max_human_num=999,\n",
    "    transform=YOLOv5Transform(img_size=640, stride=32),\n",
    "    # additional_transform={\n",
    "    #     \"sttran_inputs\": STTranTransform(img_size=600)\n",
    "    # },\n",
    "    annotation_mode=\"anticipation\",\n",
    "    train_ratio=0,\n",
    "    future_num=1,\n",
    "    future_type=\"all\",\n",
    "    future_ratio=1.0,\n",
    ")\n",
    "vidhoi_dataset_val3 = VidHOIDataset(\n",
    "    annotations_file=\"G:/datasets/VidOR/VidHOI_annotation/val_frame_annots.json\",\n",
    "    frames_dir=\"G:/datasets/VidOR/images\",\n",
    "    min_length=1,\n",
    "    max_length=6,\n",
    "    max_human_num=999,\n",
    "    transform=YOLOv5Transform(img_size=640, stride=32),\n",
    "    # additional_transform={\n",
    "    #     \"sttran_inputs\": STTranTransform(img_size=600)\n",
    "    # },\n",
    "    annotation_mode=\"anticipation\",\n",
    "    train_ratio=0,\n",
    "    future_num=3,\n",
    "    future_type=\"all\",\n",
    "    future_ratio=1.0,\n",
    ")\n",
    "vidhoi_dataset_val5 = VidHOIDataset(\n",
    "    annotations_file=\"G:/datasets/VidOR/VidHOI_annotation/val_frame_annots.json\",\n",
    "    frames_dir=\"G:/datasets/VidOR/images\",\n",
    "    min_length=1,\n",
    "    max_length=6,\n",
    "    max_human_num=999,\n",
    "    transform=YOLOv5Transform(img_size=640, stride=32),\n",
    "    # additional_transform={\n",
    "    #     \"sttran_inputs\": STTranTransform(img_size=600)\n",
    "    # },\n",
    "    annotation_mode=\"anticipation\",\n",
    "    train_ratio=0,\n",
    "    future_num=5,\n",
    "    future_type=\"all\",\n",
    "    future_ratio=1.0,\n",
    ")\n",
    "vidhoi_dataset_val7 = VidHOIDataset(\n",
    "    annotations_file=\"G:/datasets/VidOR/VidHOI_annotation/val_frame_annots.json\",\n",
    "    frames_dir=\"G:/datasets/VidOR/images\",\n",
    "    min_length=1,\n",
    "    max_length=6,\n",
    "    max_human_num=999,\n",
    "    transform=YOLOv5Transform(img_size=640, stride=32),\n",
    "    # additional_transform={\n",
    "    #     \"sttran_inputs\": STTranTransform(img_size=600)\n",
    "    # },\n",
    "    annotation_mode=\"anticipation\",\n",
    "    train_ratio=0,\n",
    "    future_num=7,\n",
    "    future_type=\"all\",\n",
    "    future_ratio=1.0,\n",
    ")\n",
    "vidhoi_dataset_val1.eval()\n",
    "vidhoi_dataset_val3.eval()\n",
    "vidhoi_dataset_val5.eval()\n",
    "vidhoi_dataset_val7.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vidhoi_dataset_val1))\n",
    "print(len(vidhoi_dataset_val3))\n",
    "print(len(vidhoi_dataset_val5))\n",
    "print(len(vidhoi_dataset_val7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = [vidhoi_dataset_val1, vidhoi_dataset_val3, vidhoi_dataset_val5, vidhoi_dataset_val7]\n",
    "set1 = set()\n",
    "set3 = set()\n",
    "set5 = set()\n",
    "set7 = set()\n",
    "all_set = [set1, set3, set5, set7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, name_set in zip(all_dataset, all_set):\n",
    "    for i in range(len(dataset)):\n",
    "        output_idx = dataset.val_window_output_map[i]\n",
    "        output_change_list = dataset.videos_change_list[output_idx]\n",
    "        video_idx = output_change_list[\"video_idx\"]\n",
    "        # index - self.val_window_acc_num[index] is the no. of sliding window\n",
    "        # + self.min_length - 1 + self.future_num to get the anticipation index\n",
    "        anticipation_im_idx = (\n",
    "            i\n",
    "            - dataset.val_window_acc_num[i]\n",
    "            + dataset.min_length\n",
    "            - 1\n",
    "            + dataset.future_num\n",
    "        )\n",
    "        video_name = dataset.video_name_list[video_idx]\n",
    "        anticipation_frame_id = dataset.frame_ids_list[video_idx][anticipation_im_idx]\n",
    "        frame_id = video_name + \"_\" + anticipation_frame_id\n",
    "        name_set.add(frame_id)\n",
    "    print(len(name_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = set1.intersection(set3, set5, set7)\n",
    "print(len(intersection))\n",
    "output = list(intersection)\n",
    "import json\n",
    "with Path(\"../vidhoi_related/common_frames_anticipation_val.json\").open(\"w\") as f:\n",
    "    json.dump(output, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hoi_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10be537dd544c2db9f613ac2c2c3348a8740d82fea605a7c3e8129db29e3148b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
