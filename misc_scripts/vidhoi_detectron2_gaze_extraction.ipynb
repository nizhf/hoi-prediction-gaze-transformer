{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert VidHOI object detection results to our format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../modules/object_tracking/yolov5\")\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import numpy as np\n",
    "import shelve\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modules.object_tracking import HeadDetection\n",
    "from modules.gaze_following import GazeFollowing\n",
    "from modules.gaze_following.head_association import assign_human_head_video\n",
    "from common.vidhoi_dataset import VidHOIDataset\n",
    "from common.data_io import FrameDatasetLoader\n",
    "from common.transforms import YOLOv5Transform\n",
    "from common.image_processing import convert_annotation_frame_to_video\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset_folder = Path(\"/mnt/DATA/datasets/VidOR\")\n",
    "\n",
    "output_folder = dataset_folder / \"VidHOI_detection\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidhoi_det_path = dataset_folder / \"VidHOI_annotation/det_val_frame_annots.json\"\n",
    "vidhoi_val_annotation_path = dataset_folder / \"VidHOI_annotation/val_frame_annots.json\"\n",
    "with vidhoi_det_path.open() as f:\n",
    "    all_detections_detectron2 = json.load(f)\n",
    "with vidhoi_val_annotation_path.open() as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "frames_to_remove = [\"1000/4925211209_000156\", \"1124/13569831214_000015\", \"1201/5658916668_000465\", \"1110/3455926688_000228\", \"1110/3455926688_000492\", \"1002/2932897373_000165\", \"1110/3455926688_000036\", \"0074/7453733046_000945\", \"1110/3455926688_000252\", \"0017/2810112808_002235\", \"0050/11587211476_001185\", \"0074/7453733046_000675\", \"1015/5919180502_001185\", \"1001/6713120511_000015\", \"1150/6104044648_000060\", \"1201/5658916668_000435\", \"1203/3345608051_001284\", \"1001/6713120511_000045\", \"1000/4925211209_000420\", \"1103/3441428429_000645\", \"0050/11587211476_001365\", \"1201/5658916668_000525\", \"1051/5979720550_000105\", \"1008/5783819683_000045\", \"1001/6713120511_000105\", \"1203/3345608051_001308\", \"1000/4925211209_000252\", \"1110/3455926688_000012\", \"1203/3345608051_001260\", \"0074/7453733046_000915\", \"1011/8627074061_001785\", \"1103/3441428429_000585\", \"1002/4103088549_000225\", \"0017/2810112808_000675\", \"0017/2810112808_001995\", \"1008/8797589693_000915\", \"1000/2716277960_000135\", \"1015/5919180502_001155\", \"0017/2810112808_000705\", \"1124/13569831214_000225\", \"0017/2810112808_002295\", \"0016/4006608424_000315\", \"1005/5991060898_000615\", \"1103/3441428429_002145\", \"1001/4889681401_001305\", \"1110/3455926688_000540\", \"0017/2810112808_000765\", \"1000/6253433310_000585\", \"0080/5522880149_000705\", \"1000/6253433310_000555\", \"0050/11587211476_001215\", \"1000/4925211209_000180\", \"1021/5352022985_000195\", \"1103/2510696559_001185\", \"0017/2810112808_000825\", \"1051/5979720550_000135\", \"1008/8797589693_001485\", \"1000/6253433310_000525\", \"1000/4925211209_000276\", \"1103/3441428429_000915\", \"0017/2810112808_000525\", \"1011/8627074061_000045\", \"1000/4925211209_000132\", \"1008/8797589693_000165\", \"1110/3455926688_000516\", \"1002/4103088549_000135\", \"1000/4925211209_000396\", \"1025/8787109801_000675\", \"0017/2810112808_002055\", \"1000/4925211209_000204\", \"1110/3455926688_000276\", \"1021/5352022985_000735\", \"0081/6139126979_000084\", \"1103/3441428429_000765\", \"0017/2810112808_002025\", \"1201/5658916668_000495\", \"1000/2716277960_000315\", \"0017/2810112808_000795\", \"1000/4925211209_000300\", \"1015/5919180502_001215\", \"1103/3441428429_001485\", \"1000/6253433310_000615\", \"0017/2810112808_002115\", \"1008/8797589693_000885\", \"1103/3441428429_000675\", \"1110/3455926688_000348\", \"0017/2810112808_000975\", \"1001/4889681401_001275\", \"1000/4925211209_000372\", \"1001/4889681401_001245\", \"1000/4925211209_000324\", \"1051/5979720550_000165\", \"0028/5840177726_000285\", \"1001/4889681401_001215\", \"1124/13569831214_000075\", \"1001/6713120511_000075\", \"1103/3441428429_001545\", \"1005/5991060898_000585\", \"1103/3441428429_001395\", \"1021/5352022985_000165\", \"1008/8797589693_000945\", \"1000/4925211209_000108\", \"0017/2810112808_000945\", \"1124/13569831214_000945\", \"0017/2810112808_002145\", \"1000/4925211209_000348\", \"0082/11503803033_000495\", \"1015/5919180502_001245\", \"1000/4925211209_000228\", \"1001/4889681401_001365\", \"1027/3113970118_000045\", \"1103/3441428429_000885\", \"1110/3455926688_000060\", \"1021/5352022985_000075\", \"0082/11503803033_000435\", \"1008/8797589693_002355\", \"1110/3455926688_000468\", \"1011/8627074061_001755\", \"1103/5521781780_001035\", \"1110/3455926688_000444\", \"1103/3441428429_000735\", \"1110/3455926688_000372\", \"1150/6104044648_000204\", \"0017/2810112808_000855\", \"1021/5352022985_000225\", \"0017/2810112808_000885\", \"1005/5991060898_000645\", \"1150/6104044648_000012\", \"1015/5919180502_001095\", \"1103/3441428429_000945\", \"1051/5979720550_000195\", \"1015/5919180502_001125\", \"1001/4889681401_001395\", \"0080/5522880149_000765\", \"1103/3441428429_000705\", \"1124/13569831214_000045\", \"1103/3441428429_001305\", \"0017/2810112808_002085\", \"1000/6253433310_000645\", \"1021/5352022985_000675\", \"1103/3441428429_001515\", \"0017/2810112808_000555\", \"1110/3455926688_000204\", \"1103/3441428429_001425\", \"0019/4759861822_000315\", \"0017/2810112808_002265\", \"1104/2821968703_000825\", \"1008/5783819683_000015\", \"1002/4103088549_000165\", \"1021/5352022985_000615\", \"1002/4103088549_000195\", \"1021/5352022985_000045\", \"1201/5658916668_000615\", \"1021/5352022985_000705\", \"1103/3441428429_000615\", \"0050/11587211476_001335\", \"0080/5522880149_000735\", \"1001/4889681401_001335\", \"0017/2810112808_002175\", \"1011/8627074061_000075\", \"1103/3441428429_001455\", \"1103/3441428429_002115\", \"0080/5522880149_000675\", \"1103/3441428429_001035\", \"0017/2810112808_000585\", \"1110/3455926688_000300\", \"1011/8627074061_000015\", \"0017/2810112808_000495\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detections = {}\n",
    "processed = set()\n",
    "for anno in annotations:\n",
    "    middle_frame_timestamp = anno['middle_frame_timestamp'] + 1\n",
    "    image_id_middle = f\"{anno['video_folder']}/{anno['video_id']}_{middle_frame_timestamp:06d}\"\n",
    "    image_id = f\"{anno['video_folder']}/{anno['video_id']}_{anno['frame_id']}\"\n",
    "    video_name = f\"{anno['video_folder']}/{anno['video_id']}\"\n",
    "\n",
    "    if image_id in processed:\n",
    "        continue\n",
    "\n",
    "    processed.add(image_id)\n",
    "\n",
    "    if video_name not in all_detections:\n",
    "        all_detections[video_name] = {\n",
    "            \"bboxes\": [],\n",
    "            \"ids\": [],\n",
    "            \"labels\": [],\n",
    "            \"confidences\": [],\n",
    "            \"frame_ids\": [],\n",
    "        }\n",
    "\n",
    "    # nothing detected in this frame\n",
    "    if image_id_middle not in all_detections_detectron2:\n",
    "        if image_id not in frames_to_remove:\n",
    "            print(image_id)\n",
    "        all_detections[video_name][\"bboxes\"].append([])\n",
    "        all_detections[video_name][\"ids\"].append([])\n",
    "        all_detections[video_name][\"labels\"].append([])\n",
    "        all_detections[video_name][\"confidences\"].append([])\n",
    "    else:\n",
    "        frame_detection = all_detections_detectron2[image_id_middle]\n",
    "        bboxes = []\n",
    "        ids = []\n",
    "        labels = []\n",
    "        confidences = []\n",
    "        for det in frame_detection:\n",
    "            bboxes.append(det[\"bbox\"])\n",
    "            ids.append(det[\"tid\"])\n",
    "            labels.append(det[\"category_id\"])\n",
    "            confidences.append(det[\"score\"])\n",
    "        all_detections[video_name][\"bboxes\"].append(bboxes)\n",
    "        all_detections[video_name][\"ids\"].append(ids)\n",
    "        all_detections[video_name][\"labels\"].append(labels)\n",
    "        all_detections[video_name][\"confidences\"].append(confidences)\n",
    "    all_detections[video_name][\"frame_ids\"].append(anno['frame_id'])\n",
    "\n",
    "for video_name in all_detections.keys():\n",
    "    frame_ids = all_detections[video_name][\"frame_ids\"]\n",
    "    if len(frame_ids) > 0:\n",
    "        sorted_idx = [i for (v, i) in sorted((v, i) for (i, v) in enumerate(frame_ids))]\n",
    "        frame_ids = [frame_ids[i] for i in sorted_idx]\n",
    "        bboxes = [all_detections[video_name][\"bboxes\"][i] for i in sorted_idx]\n",
    "        ids = [all_detections[video_name][\"ids\"][i] for i in sorted_idx]\n",
    "        labels = [all_detections[video_name][\"labels\"][i] for i in sorted_idx]\n",
    "        confidences = [all_detections[video_name][\"confidences\"][i] for i in sorted_idx]\n",
    "        all_detections[video_name][\"frame_ids\"] = frame_ids\n",
    "        all_detections[video_name][\"bboxes\"] = bboxes\n",
    "        all_detections[video_name][\"ids\"] = ids\n",
    "        all_detections[video_name][\"labels\"] = labels\n",
    "        all_detections[video_name][\"confidences\"] = confidences\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_out = output_folder / \"val_trace_detectron2.json\"\n",
    "with det_out.open(\"w\") as out:\n",
    "    json.dump(all_detections, out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on VidHOI Detectron2 results, apply Gaze Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Head Tracking and Gaze Following modules\n",
    "head_detection_module = HeadDetection(\n",
    "    crowd_human_weight_path=\"../weights/yolov5/crowdhuman_yolov5m.pt\",\n",
    "    config_path=\"../configs/object_tracking.yaml\",\n",
    "    device=device,\n",
    ")\n",
    "gaze_following_module = GazeFollowing(\n",
    "    weight_path=\"../weights/detecting_attended/model_videoatttarget.pt\",\n",
    "    config_path=\"../configs/gaze_following.yaml\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "img_size = 640\n",
    "yolov5_stride = head_detection_module.yolov5_stride\n",
    "# NOTE adjust this tolerance and method\n",
    "head_matching_iou_thres = 0.7\n",
    "head_matching_method = \"hungarian\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidhoi_val_dataset = VidHOIDataset(\n",
    "    annotations_file=\"/mnt/DATA/datasets/VidOR/VidHOI_annotation/val_frame_annots.json\",\n",
    "    frames_dir=\"/mnt/DATA/datasets/VidOR/images\",\n",
    "    transform=YOLOv5Transform(img_size, yolov5_stride),\n",
    "    min_length=1,\n",
    "    max_length=999999,\n",
    "    max_human_num=999999,\n",
    "    annotation_mode=\"clip\",\n",
    ")\n",
    "vidhoi_val_dataloader = DataLoader(vidhoi_val_dataset, batch_size=None, shuffle=False)\n",
    "\n",
    "output_val_head_filename = str(output_folder / \"val_frame_heads_detectron2\")\n",
    "output_val_gaze_filename = str(output_folder / \"val_frame_gazes_detectron2\")\n",
    "output_val_inout_filename = str(output_folder / \"val_frame_inout_detectron2\")\n",
    "\n",
    "det_out = output_folder / \"val_trace_detectron2.json\"\n",
    "with det_out.open(\"r\") as out:\n",
    "    all_detections = json.load(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_val_head_dict = shelve.open(output_val_head_filename)\n",
    "output_val_gaze_dict = shelve.open(output_val_gaze_filename)\n",
    "output_val_inout_dict = shelve.open(output_val_inout_filename)\n",
    "# For each video, first detect heads\n",
    "t = tqdm(vidhoi_val_dataloader)\n",
    "for frames, annotations, meta_info in t:\n",
    "    video_name = meta_info[\"video_name\"]\n",
    "    t.set_description(f\"{video_name}\")\n",
    "    t.refresh()\n",
    "    original_frames = meta_info[\"original_frames\"]\n",
    "    # bboxes from detection\n",
    "    clip_detections = all_detections[video_name]\n",
    "    # convert to [im_idx, x1, y1, x2, y2] format\n",
    "    bboxes, ids, labels, _ = convert_annotation_frame_to_video(\n",
    "        clip_detections[\"bboxes\"],\n",
    "        clip_detections[\"ids\"],\n",
    "        clip_detections[\"labels\"],\n",
    "        clip_detections[\"confidences\"],\n",
    "    )\n",
    "    # detect head and assign to human\n",
    "    with torch.no_grad():\n",
    "        video_head_bbox_list = assign_human_head_video(\n",
    "            frames,\n",
    "            original_frames,\n",
    "            bboxes,\n",
    "            ids,\n",
    "            labels,\n",
    "            head_detection_module,\n",
    "            head_matching_iou_thres,\n",
    "            device,\n",
    "            method=head_matching_method,\n",
    "        )\n",
    "    # assign video head bbox list to its name\n",
    "    output_val_head_dict[video_name] = video_head_bbox_list\n",
    "    output_val_head_dict.sync()\n",
    "\n",
    "    # for each head bbox, detect gaze\n",
    "    video_gaze_list = []\n",
    "    video_inout_list = []\n",
    "    hx_memory = {}\n",
    "    for i, (head_bboxes, frame0) in enumerate(\n",
    "        zip(video_head_bbox_list, original_frames)\n",
    "    ):\n",
    "        t.set_description(\n",
    "            f\"{video_name}/{meta_info['frame_ids'][i]}, {i}/{len(video_head_bbox_list) - 1}: \"\n",
    "        )\n",
    "        t.refresh()\n",
    "        frame_gaze_dict = {}\n",
    "        frame_inout_dict = {}\n",
    "        for human_id, head_bbox in head_bboxes.items():\n",
    "            t.set_postfix_str(f\"{head_bbox}\")\n",
    "            t.refresh()\n",
    "            # no head found for this human_id\n",
    "            if len(head_bbox) == 0:\n",
    "                frame_gaze_dict[human_id] = []\n",
    "                frame_inout_dict[human_id] = []\n",
    "                continue\n",
    "            # check hidden state memory\n",
    "            if human_id in hx_memory:\n",
    "                hidden_state = hx_memory[human_id]\n",
    "            else:\n",
    "                hidden_state = None\n",
    "            with torch.no_grad():\n",
    "                (heatmap, inout, hx, _, _, _,) = gaze_following_module.detect_one(\n",
    "                    frame0.numpy(),\n",
    "                    head_bbox,\n",
    "                    hidden_state,\n",
    "                    draw=False,\n",
    "                )\n",
    "            hx_memory[human_id] = (hx[0].detach(), hx[1].detach())\n",
    "            # process heatmap 64x64 (not include inout), store inout info separately\n",
    "            # softmax inout, value = probability of gaze inside the scene\n",
    "            inout_modulated = 1 / (1 + np.exp(-inout))\n",
    "            # assign heatmap and in_out to human_id\n",
    "            frame_gaze_dict[human_id] = heatmap\n",
    "            frame_inout_dict[human_id] = inout_modulated\n",
    "        # append frame heatmap and inout dict to video heatmap list\n",
    "        video_gaze_list.append(frame_gaze_dict)\n",
    "        video_inout_list.append(frame_inout_dict)\n",
    "    # assign video heatmap list to its name\n",
    "    output_val_gaze_dict[video_name] = video_gaze_list\n",
    "    output_val_gaze_dict.sync()\n",
    "    output_val_inout_dict[video_name] = video_inout_list\n",
    "    output_val_inout_dict.sync()\n",
    "\n",
    "output_val_head_dict.close()\n",
    "output_val_gaze_dict.close()\n",
    "output_val_inout_dict.close()\n",
    "print(f\"Head bboxes dumped to {output_val_head_filename}\")\n",
    "print(f\"Gaze heatmaps dumped to {output_val_gaze_filename}\")\n",
    "print(f\"Gaze inout dumped to {output_val_inout_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hoi_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10be537dd544c2db9f613ac2c2c3348a8740d82fea605a7c3e8129db29e3148b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
