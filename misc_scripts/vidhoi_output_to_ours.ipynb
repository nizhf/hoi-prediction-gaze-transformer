{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert VidHOI baseline output to our format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "vidhoi_output_path = Path(\"../../VidHOI/output/SLOWFAST_32x2_R50_SHORT_SCRATCH_EVAL_GT_trajectory-toipool-spa_conf/all_results_vidor_checkpoint_epoch_00020.pyth_proposal_less-168-examples.json\")\n",
    "\n",
    "output_path = Path(\"../../runs/sttran_gaze_vidhoi/vidhoi_baseline/eval/all_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with vidhoi_output_path.open() as f:\n",
    "    vidhoi_results = json.load(f)\n",
    "\n",
    "print(vidhoi_results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for vidhoi_result in tqdm(vidhoi_results):\n",
    "    bboxes = [b[1:] for b in vidhoi_result[\"proposal_boxes\"]]\n",
    "    pred_labels = [int(l[1]) for l in vidhoi_result[\"proposal_classes\"]]\n",
    "    confidences = [c[1] for c in vidhoi_result[\"proposal_scores\"]]\n",
    "    pair_idxes = vidhoi_result[\"preds_bbox_pair_ids\"]\n",
    "    interaction_distribution = vidhoi_result[\"preds_score\"]\n",
    "    bboxes_gt = [bg[1:] for bg in vidhoi_result[\"gt_boxes\"]]\n",
    "    labels_gt = [int(lg[1]) for lg in vidhoi_result[\"gt_obj_classes\"]]\n",
    "    ids_gt = [i for i in range(len(bboxes_gt))]\n",
    "    pair_idxes_gt = vidhoi_result[\"gt_bbox_pair_ids\"]\n",
    "    interactions_gt = vidhoi_result[\"gt_action_labels\"]\n",
    "\n",
    "    result = {\n",
    "        \"bboxes\": bboxes,  # detected bboxes, [x1, y1, x2, y2]\n",
    "        \"pred_labels\": pred_labels,  # detected labels\n",
    "        \"confidences\": confidences,  # detection confidences\n",
    "        \"pair_idxes\": pair_idxes,  # all detected pairs\n",
    "        \"interaction_distribution\": interaction_distribution,\n",
    "        \"bboxes_gt\": bboxes_gt,  # ground-truth object bboxes\n",
    "        \"labels_gt\": labels_gt,  # ground-truth object labels\n",
    "        \"ids_gt\": ids_gt,  # ground-truth ids, important for anticipation\n",
    "        \"pair_idxes_gt\": pair_idxes_gt,  # gt pair idxes\n",
    "        \"interactions_gt\": interactions_gt,  # gt interactions\n",
    "    }\n",
    "\n",
    "    all_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with output_path.open(\"w\") as out:\n",
    "    json.dump(all_results, out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine which 168 frames to delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "vidhoi_168_path = Path(\"/mnt/DATA/datasets/VidOR/VidHOI_annotation/val_instances_predictions_train_small_vidor_with_pseudo_labels.pth\")\n",
    "vidhoi_det_168_path = Path(\"/mnt/DATA/datasets/VidOR/VidHOI_annotation/det_val_frame_annots.json\")\n",
    "vidhoi_val_annotation_path = Path(\"/mnt/DATA/datasets/VidOR/VidHOI_annotation/val_frame_annots.json\")\n",
    "\n",
    "pseudo_detections = torch.load(str(vidhoi_168_path))\n",
    "with vidhoi_det_168_path.open() as f:\n",
    "    all_detections = json.load(f)\n",
    "with vidhoi_val_annotation_path.open() as f:\n",
    "    annotations = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_detections))\n",
    "print(len(pseudo_detections))\n",
    "print(len(annotations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = [det[\"image_id\"] for det in pseudo_detections]\n",
    "set1 = set(set1)\n",
    "set2 = all_detections.keys()\n",
    "set2 = set(set2)\n",
    "\n",
    "print(len(set1))\n",
    "print(len(set2))\n",
    "\n",
    "frames_to_removed = set1 - set2 \n",
    "print(len(frames_to_removed))\n",
    "print(frames_to_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = set()\n",
    "for anno in annotations:\n",
    "    middle_frame_timestamp = anno['middle_frame_timestamp'] + 1\n",
    "    image_id_middle = f\"{anno['video_folder']}/{anno['video_id']}_{middle_frame_timestamp:06d}\"\n",
    "    if image_id_middle in frames_to_removed:\n",
    "        image_id = f\"{anno['video_folder']}/{anno['video_id']}_{anno['frame_id']}\"\n",
    "        to_delete.add(image_id)\n",
    "\n",
    "print(len(to_delete))\n",
    "print(to_delete)\n",
    "\n",
    "output_path = Path(\"../vidhoi_related/168_frames_to_remove.json\")\n",
    "with output_path.open(\"w\") as f:\n",
    "    json.dump(list(to_delete), f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('hoi_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10be537dd544c2db9f613ac2c2c3348a8740d82fea605a7c3e8129db29e3148b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
